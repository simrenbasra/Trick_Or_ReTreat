{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Environment: Simple Haunted Mansion\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Game Information\n",
    "\n",
    "For this notebook, I will be simplifying the scope of the environment. In the simple Haunted Mansion, there are no ghosts or candies, the only reward is the exit door.\n",
    "\n",
    "Below is further information:\n",
    "\n",
    "    - There are 4 discrete actions the agent can take: up. down, left and right\n",
    "    - The game terminates when the agent has reached the exit door of the haunted mansion\n",
    "    - To keep it simple, the agent is only to receive a reward of + 1 when they have reached the exit door\n",
    "    - The agent represents the trick-or-treater\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "#import gym \n",
    "import pygame\n",
    "from simple_env import Simple_Haunted_Mansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gymnasium has some code to help create custom environments, link: https://gymnasium.farama.org/introduction/create_custom_env/\n",
    "\n",
    "I have used this code as the basis for my custom environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "gym.register(\n",
    "    id='Haunted_Mansion-v1',\n",
    "    entry_point='simple_env:Simple_Haunted_Mansion'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the custom env, set the render_mode to huamn\n",
    "env = gym.make('Haunted_Mansion-v1',  render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Movement (Testing of the Rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the environment, to set the agent to a random starting place on the grid\n",
    "env.reset()\n",
    "\n",
    "# Run the environment for 50 random steps\n",
    "for i in range(50):\n",
    "\n",
    "    # Using sample() to pick random action for agent to take\n",
    "    action = env.action_space.sample()\n",
    "    # Calling step to get information on env after action takes the random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Rendering env to see the agent's movement\n",
    "    env.render()\n",
    "    \n",
    "    # Add time delay in pygame to pause after each action, allows for better viewing\n",
    "    pygame.time.delay(350)\n",
    "\n",
    "    # If the agent has reached the target, break out of the loop and end the episode\n",
    "    if terminated:\n",
    "        break\n",
    "        \n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with PPO Algorithm\n",
    "----\n",
    "\n",
    "Deciding not to store logs at the moment as environment is fairly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stable baselines 3 for PPO and Wrappers\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id='Haunted_Mansion-v1',\n",
    "    entry_point='simple_env:Simple_Haunted_Mansion'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and vectorise it using DummyVecEnv\n",
    "v_env = DummyVecEnv([lambda: gym.make('Haunted_Mansion-v1', render_mode = None) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 13665 |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 0     |\n",
      "|    total_timesteps | 6144  |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 6861        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019695742 |\n",
      "|    clip_fraction        | 0.351       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | -0.104      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0125      |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0355     |\n",
      "|    value_loss           | 0.123       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5805        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017514966 |\n",
      "|    clip_fraction        | 0.395       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.247       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0213     |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0488     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5239        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023463404 |\n",
      "|    clip_fraction        | 0.407       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | 0.15        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0518     |\n",
      "|    value_loss           | 0.0503      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5067        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033040676 |\n",
      "|    clip_fraction        | 0.457       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.00625     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0551     |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0553     |\n",
      "|    value_loss           | 0.0104      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4985        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024495443 |\n",
      "|    clip_fraction        | 0.355       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.936      |\n",
      "|    explained_variance   | -0.0191     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0566     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0505     |\n",
      "|    value_loss           | 0.0015      |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4934        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029089445 |\n",
      "|    clip_fraction        | 0.302       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.758      |\n",
      "|    explained_variance   | 0.341       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0403     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.052      |\n",
      "|    value_loss           | 0.00037     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 4893       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03390947 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.575     |\n",
      "|    explained_variance   | 0.621      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0593    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0413    |\n",
      "|    value_loss           | 0.00015    |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 4862       |\n",
      "|    iterations           | 9          |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03098996 |\n",
      "|    clip_fraction        | 0.0988     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.456     |\n",
      "|    explained_variance   | 0.828      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.051     |\n",
      "|    n_updates            | 80         |\n",
      "|    policy_gradient_loss | -0.0294    |\n",
      "|    value_loss           | 5.65e-05   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 4838       |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01115597 |\n",
      "|    clip_fraction        | 0.0799     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.377     |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0217    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0198    |\n",
      "|    value_loss           | 2.18e-05   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4818         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073141954 |\n",
      "|    clip_fraction        | 0.0459       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.345       |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00572     |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.0115      |\n",
      "|    value_loss           | 1.16e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4802        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010483876 |\n",
      "|    clip_fraction        | 0.0816      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.332      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00662    |\n",
      "|    value_loss           | 2.46e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4789        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006738882 |\n",
      "|    clip_fraction        | 0.0398      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000142    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00187    |\n",
      "|    value_loss           | 2.75e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4778        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008133805 |\n",
      "|    clip_fraction        | 0.0973      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.335      |\n",
      "|    explained_variance   | 0.983       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0194     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00525    |\n",
      "|    value_loss           | 2.98e-05    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4766        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006190479 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.334      |\n",
      "|    explained_variance   | 0.968       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0387      |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    value_loss           | 2.89e-06    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4748         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040908153 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.338       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0442       |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | 0.00211      |\n",
      "|    value_loss           | 2.94e-05     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 4737       |\n",
      "|    iterations           | 17         |\n",
      "|    time_elapsed         | 22         |\n",
      "|    total_timesteps      | 104448     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00958022 |\n",
      "|    clip_fraction        | 0.0425     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.358     |\n",
      "|    explained_variance   | 0.996      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0263    |\n",
      "|    n_updates            | 160        |\n",
      "|    policy_gradient_loss | -0.00322   |\n",
      "|    value_loss           | 8.62e-06   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x29024af10>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise PPO model\n",
    "# Having to use MultiInputPolicy instead of MlpPolicy since observations are of type dict\n",
    "model = PPO('MultiInputPolicy', v_env, verbose=1)\n",
    "\n",
    "# Train the agent for 100_000 time steps\n",
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Training/Saved Models/PPO_simple_haunted_mansion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing after PPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register(\n",
    "    id='Haunted_Mansion-v1',\n",
    "    entry_point='simple_env:Simple_Haunted_Mansion'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Haunted_Mansion-v1', render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: 1\n",
      "Episode 2 Score: 1\n",
      "Episode 3 Score: 1\n",
      "Episode 4 Score: 1\n",
      "Episode 5 Score: 1\n",
      "Episode 6 Score: 1\n",
      "Episode 7 Score: 1\n",
      "Episode 8 Score: 1\n",
      "Episode 9 Score: 1\n",
      "Episode 10 Score: 1\n",
      "Episode 11 Score: 1\n",
      "Episode 12 Score: 1\n",
      "Episode 13 Score: 1\n",
      "Episode 14 Score: 1\n",
      "Episode 15 Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of episodes for testing\n",
    "episodes = 15\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "   \n",
    "    # Resetting the environment to an initial state\n",
    "    obs, info_= env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from the trained model\n",
    "        action, _ = model.predict(obs) \n",
    "        \n",
    "        # Step through the environment using the action\n",
    "        obs, reward, terminated,truncated, info = env.step(action)  \n",
    "               \n",
    "        # Update the done condition based on terminated flag (truncated is always False in this env)\n",
    "        done = terminated\n",
    "\n",
    "        # Visualise the environment after each action agent takes\n",
    "        env.render()\n",
    "        \n",
    "        # Delay to slow down the movement for better observation\n",
    "        pygame.time.delay(350) \n",
    "    \n",
    "    # After the episode ends, print the score\n",
    "    print(f'Episode {episode} Score: {reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Comment:**\n",
    "\n",
    "After seeing the agent's actions via pygame, its clear the agent doesn't always take the most optimal path, to get the agent to do this I can add a penalty to the number of timesteps taken to reach the target. This is something I will implement by adding a penalty of 0.1 for every action which did not result in finding the target location.\n",
    "\n",
    "To do this I will add a else case into the if statement in the step function for reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much point in running the evaluate_policy since reward system is quite simple. Across all episodes the agent was able to get to the door and receive the reward of 1.\n",
    "\n",
    "Since there is no penalty there is no real incentive for the agent to learn the most optimal path to the exit door. To add penalty to the reward to force the agent to start finding the optimal path. I will be adding this in in the next iteration of my custom environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have successfully created a simple custom environment for Trick or ReTreat with rendering in pygame. \n",
    "\n",
    "After training the agent using PPO, it is clear the agent has learnt to escape the mansion but fails to find the most optimal path. In the next notebook I will be making the following changes to the environment:\n",
    "\n",
    "    - adding penalty for each timestep the agent does not reach its target(door).\n",
    "    - adding in ghosts and candies to see if the agent can find an optimal path while maximsing its rewards.\n",
    "    - implement Q-learning instead of PPO to deal with the added complexity.\n",
    "    - compare PPO model and Q-learning training to see which trains the agent better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trick_or_retreat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
