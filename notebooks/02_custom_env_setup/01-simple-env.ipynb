{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Environment: Simple Haunted Mansion\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Game Information\n",
    "\n",
    "For this notebook, I will be simplifying the scope of the environment. In the simple Haunted Mansion, there are no ghosts or candies, the only reward is the exit door.\n",
    "\n",
    "Below is further information:\n",
    "\n",
    "    - There are 4 discrete actions the agent can take: up. down, left and right\n",
    "    - The game terminates when the agent has reached the exit door of the haunted mansion\n",
    "    - To keep it simple, the agent is only to receive a reward of + 1 when they have reached the exit door\n",
    "    - The agent represents the trick-or-treater\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "#import gym \n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Custom Environment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gymnasium has some code to help create custom environments, link: https://gymnasium.farama.org/introduction/create_custom_env/\n",
    "\n",
    "I have used this code as the basis for my custom environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Haunted_Mansion(gym.Env):\n",
    "\n",
    "    # Defining metadata (render_modes/render_fps)\n",
    "    metadata = {'render_modes' : ['human'], 'render_fps': 1}\n",
    "\n",
    "    ##########################################################################\n",
    "    # Init\n",
    "    ##########################################################################\n",
    "\n",
    "    def __init__(self, size: int = 5, render_mode = 'human'):\n",
    "        ''' \n",
    "        Description:\n",
    "            Initialises the environment\n",
    "\n",
    "        Inputs:\n",
    "            size: int \n",
    "                The grid size, 5 by 5 for default \n",
    "\n",
    "            render_mode: str\n",
    "                For visualisation, default render mode set to 'human'\n",
    "\n",
    "        Outputs:\n",
    "            size : int\n",
    "                The size of the grid, which will be a square of `size x size`.\n",
    "                \n",
    "            render_mode : str\n",
    "                The rendering mode used by the environment.\n",
    "                \n",
    "            num_rows : int\n",
    "                Number of rows in the grid, equal to `size`.\n",
    "                \n",
    "            num_cols : int\n",
    "                Number of columns in the grid, equal to `size`.\n",
    "                \n",
    "            agent_location : array\n",
    "                Initial location of the agent as a numpy array.\n",
    "                \n",
    "            target_location : array\n",
    "                Fixed location of the target/door as a numpy array, set to [4, 4].\n",
    "                \n",
    "            observation_space : gym.spaces.Dict\n",
    "                Observation space for the environment, containing the agent's and target's grid positions.\n",
    "                \n",
    "            action_space : gym.spaces.Discrete\n",
    "                Action space with four discrete actions: right, up, left, down.\n",
    "                \n",
    "            action_to_direction : dict\n",
    "                Dictionary that maps actions.\n",
    "                \n",
    "            screen_size : int \n",
    "                The pixel size of the screen for displaying the grid; defaults to 800.\n",
    "                \n",
    "            cell_size : int \n",
    "                The pixel size of each cell in the grid.\n",
    "        '''\n",
    "\n",
    "        # Setting size of grid to size input parameter\n",
    "        self.size = size   \n",
    "\n",
    "        # Setting render mode to render_mode input parameter    \n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        # Setting number of rows and columns of grid using size\n",
    "        self.num_rows, self.num_cols = self.size, self.size\n",
    "\n",
    "        # Placeholder value for agent location, the agent is out of bounds and is randomly set on the grid during reset() function\n",
    "        self.agent_location = np.array([-1, -1], dtype=np.int64)\n",
    "\n",
    "        # Setting position of the target_location (exit door), the door is static\n",
    "        self.target_location = np.array([4, 4], dtype=np.int64)\n",
    "\n",
    "        # Observations are represented as dictionaries with the agent's and the target's location.\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                'agent': gym.spaces.Box(0, size - 1, shape=(2,), dtype = np.int64),\n",
    "                'target': gym.spaces.Box(0, size - 1, shape=(2,), dtype = np.int64),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # We have 4 actions: right, up, left, down\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Dictionary to map the actions to directions on the grid\n",
    "        self.action_to_direction = {\n",
    "            0: np.array([1, 0]),  # right\n",
    "            1: np.array([0, 1]),  # up\n",
    "            2: np.array([-1, 0]),  # left\n",
    "            3: np.array([0, -1]),  # down\n",
    "        }\n",
    "        \n",
    "        # Initialise Pygame if render_mode is 'human'\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            pygame.init()\n",
    "            self.screen_size = 800\n",
    "            self.cell_size = self.screen_size // self.size\n",
    "            self.screen = pygame.display.set_mode((self.screen_size, self.screen_size))\n",
    "\n",
    "            pygame.display.set_caption('Trick or ReTreat: Escape the Mansion!')\n",
    "\n",
    "\n",
    "    ##########################################################################\n",
    "    # Returning Observations\n",
    "    ##########################################################################\n",
    "\n",
    "    def _get_obs(self):\n",
    "        ''' \n",
    "        Description:\n",
    "            Returns environment observations based on agents location. \n",
    "\n",
    "        Outputs:\n",
    "            observations: dict\n",
    "                Returns agent's and target's location.\n",
    "        '''\n",
    "        return {'agent': self.agent_location, 'target': self.target_location}\n",
    "\n",
    "    ##########################################################################\n",
    "    # Returning Distance (between the agent and door)\n",
    "    ##########################################################################\n",
    "\n",
    "    def _get_info(self):\n",
    "        ''' \n",
    "        Description:\n",
    "            Returns environment information based on agents location (door). \n",
    "\n",
    "        Outputs:\n",
    "            information: \n",
    "                Returns distance between agent and target location (door).\n",
    "        '''\n",
    "        return {\n",
    "            'distance': np.linalg.norm(\n",
    "                self.agent_location - self.target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    ##########################################################################\n",
    "    # Resetting the Environment\n",
    "    ##########################################################################\n",
    "\n",
    "    def reset(self, seed:int = None, options: dict = None):\n",
    "        ''' \n",
    "        Description:\n",
    "            Resets environment to an initial state.\n",
    "\n",
    "        Inputs:\n",
    "            seed: int \n",
    "                Control randomness, set to None as default.\n",
    "\n",
    "        Outputs:\n",
    "            information: \n",
    "                Returns initial observation and info of environmend based on agent's starting location\n",
    "        '''\n",
    "        # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Setting the agents starting location randomly on the grid\n",
    "        self.agent_location = self.np_random.integers(0, self.size, size=2, dtype= np.int64)\n",
    "        \n",
    "        # Getting initial observations and info based on starting agent position\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self.render()\n",
    "        \n",
    "        return observation, info\n",
    "\n",
    "    ##########################################################################\n",
    "    # Step\n",
    "    ##########################################################################  \n",
    "    \n",
    "    def step(self, action):\n",
    "\n",
    "        ''' \n",
    "        Description:\n",
    "            To get observation, reward, terminated, truncated and info once agent has taken an action.\n",
    "\n",
    "        Inputs:\n",
    "            action: int \n",
    "                Control randomness, set to None as default.\n",
    "\n",
    "        Outputs:\n",
    "            observation:\n",
    "                Returns observation (agent, target location) of environment based on action agent has taken.\n",
    "\n",
    "            reward:\n",
    "                Points received when agent reaches the target.\n",
    "            \n",
    "            terminated:\n",
    "                Boolean flag, set to True only if the the agent reaches the target(door).\n",
    "\n",
    "            truncated:\n",
    "                Boolean flag, set to False always (simple environment).\n",
    "\n",
    "            info:\n",
    "                Returns info of environment based on action agent has taken.\n",
    "        '''\n",
    "\n",
    "        # Converting action to int\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = np.int64(action.item()) \n",
    "\n",
    "        direction = self.action_to_direction[action]\n",
    "        \n",
    "        # We use np.clip to make sure we don't leave the grid bounds\n",
    "        self.agent_location = np.clip(\n",
    "            self.agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # No time out for steps, will set truncated to False\n",
    "        truncated = False\n",
    "\n",
    "        # Terminated only when reward is reached (agent same location as door)\n",
    "        terminated =np.array_equal(self.agent_location, self.target_location)\n",
    "\n",
    "        # To only receive reward of 1 if terminated flag is set\n",
    "        reward = 1 if terminated else 0\n",
    "\n",
    "        # Get observation and info after taking an action\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    ##########################################################################\n",
    "    # Render\n",
    "    ##########################################################################  \n",
    "   \n",
    "    def render(self):\n",
    "        ''' \n",
    "        Description:\n",
    "            To visualise the environment and agent's actions, only rendering mode is for human.\n",
    "\n",
    "        Outputs:\n",
    "            pygame display window depicting grid, agent's movement and target location.\n",
    "        '''\n",
    "                \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                exit()\n",
    "\n",
    "        # Set background to all white\n",
    "        self.screen.fill((255, 255, 255))\n",
    "\n",
    "        # Looping through rows and columns to draw rectanges (representing the grid)\n",
    "        for row in range(self.size):\n",
    "            for col in range(self.size):\n",
    "                # Calculates x and y positions of each cell in the grid by multiplying the col/row index by pixel cell size\n",
    "                cell_x = col * self.cell_size\n",
    "                cell_y = row * self.cell_size\n",
    "                # Drawing white rectangle to represent each cell in the grid\n",
    "                pygame.draw.rect(self.screen, (0, 0, 0), (cell_x, cell_y, self.cell_size, self.cell_size), 1)\n",
    "\n",
    "        # To calculate the offset to ensure images are placed in centre of cells\n",
    "        offset = self.cell_size * 0.1\n",
    "\n",
    "        # Transform target grid cooridinates into pixel coordinates \n",
    "        door_pos = self.target_location * self.cell_size\n",
    "        \n",
    "        # Representing the door as an image from Canva\n",
    "        door_img = pygame.image.load('images/Door.png')\n",
    "        # Scaling the image of the Door to be smaller than size of the cell\n",
    "        door_img = pygame.transform.scale(door_img,(self.cell_size * 0.8,self.cell_size * 0.8))\n",
    "        # Drawing the image to the grid, adding offset to ensure img is in the middle\n",
    "        self.screen.blit(door_img, (door_pos[1] + offset , door_pos[0] + offset))\n",
    "\n",
    "        agent_pos = self.agent_location * self.cell_size\n",
    "        # Representing the agent as an image from Canva\n",
    "        agent_img = pygame.image.load('images/Agent.png')\n",
    "        agent_img = pygame.transform.scale(agent_img,(self.cell_size * 0.8,self.cell_size * 0.8))\n",
    "        self.screen.blit(agent_img, (agent_pos[1] + offset, agent_pos[0] + offset))\n",
    "\n",
    "        # To keep updating the display after each action\n",
    "        pygame.display.update()  \n",
    "\n",
    "    ##########################################################################\n",
    "    # Close\n",
    "    ##########################################################################  \n",
    "\n",
    "    def close(self):\n",
    "        ''' \n",
    "        Description:\n",
    "            To close environment.\n",
    "\n",
    "        Outputs:\n",
    "            Quit all pygame windows after the environment is no longer in use.\n",
    "        '''\n",
    "        \n",
    "        if self.render_mode == 'human':\n",
    "            pygame.quit()  # Close the pygame window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "gym.register(\n",
    "    id='Haunted_Mansion-v1',\n",
    "    entry_point='__main__:Haunted_Mansion'  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the custom env, set the render_mode to huamn\n",
    "env = gym.make('Haunted_Mansion-v1',  render_mode='human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Movement (Testing of the Rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetting the environment, to set the agent to a random starting place on the grid\n",
    "env.reset()\n",
    "\n",
    "# Run the environment for 50 random steps\n",
    "for i in range(50):\n",
    "\n",
    "    # Using sample() to pick random action for agent to take\n",
    "    action = env.action_space.sample()\n",
    "    # Calling step to get information on env after action takes the random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # Rendering env to see the agent's movement\n",
    "    env.render()\n",
    "    \n",
    "    # Add time delay in pygame to pause after each action, allows for better viewing\n",
    "    pygame.time.delay(350)\n",
    "\n",
    "    # If the agent has reached the target, break out of the loop and end the episode\n",
    "    if terminated:\n",
    "        break\n",
    "        \n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with PPO Algorithm\n",
    "----\n",
    "\n",
    "Deciding not to store logs at the moment as environment is fairly simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing stable baselines 3 for PPO and Wrappers\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simybasra/anaconda3/envs/trick_or_retreat/lib/python3.9/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Haunted_Mansion-v1 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Registering the environemnt \n",
    "gym.register(\n",
    "    id='Haunted_Mansion-v1',\n",
    "    entry_point='__main__:Haunted_Mansion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment and vectorise it using DummyVecEnv\n",
    "v_env = DummyVecEnv([lambda: gym.make('Haunted_Mansion-v1', render_mode = None) for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 13014 |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 0     |\n",
      "|    total_timesteps | 6144  |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 6632        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019906264 |\n",
      "|    clip_fraction        | 0.312       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.022       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00794     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0357     |\n",
      "|    value_loss           | 0.114       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5807        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015692962 |\n",
      "|    clip_fraction        | 0.356       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.223       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0107      |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0473     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5467        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021193191 |\n",
      "|    clip_fraction        | 0.417       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.241       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0592     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0532     |\n",
      "|    value_loss           | 0.0325      |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 5258       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 5          |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03874803 |\n",
      "|    clip_fraction        | 0.439      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.1       |\n",
      "|    explained_variance   | -0.357     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0514    |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0468    |\n",
      "|    value_loss           | 0.00457    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5138        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026563773 |\n",
      "|    clip_fraction        | 0.374       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.924      |\n",
      "|    explained_variance   | 0.0961      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0439     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.056      |\n",
      "|    value_loss           | 0.000993    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 5055        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046588708 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.723      |\n",
      "|    explained_variance   | 0.422       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0693     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0545     |\n",
      "|    value_loss           | 0.000377    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 4992       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03515519 |\n",
      "|    clip_fraction        | 0.226      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.541     |\n",
      "|    explained_variance   | 0.633      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0643    |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0425    |\n",
      "|    value_loss           | 0.000149   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4943        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.053724926 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.425      |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.07       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 6.77e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4905         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053331945 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.379       |\n",
      "|    explained_variance   | 0.96         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0353      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00796     |\n",
      "|    value_loss           | 2.73e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4867         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059688874 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.367       |\n",
      "|    explained_variance   | 0.972        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0122      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00777     |\n",
      "|    value_loss           | 2.81e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4842         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086372495 |\n",
      "|    clip_fraction        | 0.0684       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.378       |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0728       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00379     |\n",
      "|    value_loss           | 1.71e-05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4822         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030926384 |\n",
      "|    clip_fraction        | 0.0316       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.337       |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0201      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00504     |\n",
      "|    value_loss           | 4.96e-05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4806        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006852627 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.358      |\n",
      "|    explained_variance   | 0.986       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0228     |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00485    |\n",
      "|    value_loss           | 4.38e-05    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4791         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 92160        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057425373 |\n",
      "|    clip_fraction        | 0.0656       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.334       |\n",
      "|    explained_variance   | 0.991        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0136      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00077     |\n",
      "|    value_loss           | 5.12e-06     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 4779         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063708127 |\n",
      "|    clip_fraction        | 0.0509       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.313       |\n",
      "|    explained_variance   | 0.99         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0149      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 8.27e-06     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 4768        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005144114 |\n",
      "|    clip_fraction        | 0.0658      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.351      |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000822   |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.000857    |\n",
      "|    value_loss           | 2.18e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x281911be0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialise PPO model\n",
    "# Having to use MultiInputPolicy instead of MlpPolicy since observations are of type dict\n",
    "model = PPO('MultiInputPolicy', v_env, verbose=1)\n",
    "\n",
    "# Train the agent for 100_000 time steps\n",
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../../Training/Saved Models/PPO_simple_haunted_mansion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing after PPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simybasra/anaconda3/envs/trick_or_retreat/lib/python3.9/site-packages/gymnasium/envs/registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment Haunted_Mansion-v1 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# Registering the environemnt \n",
    "gym.register(\n",
    "    id='Haunted_Mansion-v1',\n",
    "    entry_point='__main__:Haunted_Mansion'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Haunted_Mansion-v1', render_mode = 'human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: 1\n",
      "Episode 2 Score: 1\n",
      "Episode 3 Score: 1\n",
      "Episode 4 Score: 1\n",
      "Episode 5 Score: 1\n",
      "Episode 6 Score: 1\n",
      "Episode 7 Score: 1\n",
      "Episode 8 Score: 1\n",
      "Episode 9 Score: 1\n",
      "Episode 10 Score: 1\n",
      "Episode 11 Score: 1\n",
      "Episode 12 Score: 1\n",
      "Episode 13 Score: 1\n",
      "Episode 14 Score: 1\n",
      "Episode 15 Score: 1\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of episodes for testing\n",
    "episodes = 15\n",
    "\n",
    "for episode in range(1, episodes + 1):\n",
    "   \n",
    "    # Resetting the environment to an initial state\n",
    "    obs, info_= env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # Get action from the trained model\n",
    "        action, _ = model.predict(obs) \n",
    "        \n",
    "        # Step through the environment using the action\n",
    "        obs, reward, terminated,truncated, info = env.step(action)  \n",
    "               \n",
    "        # Update the done condition based on terminated flag (truncated is always False in this env)\n",
    "        done = terminated\n",
    "\n",
    "        # Visualise the environment after each action agent takes\n",
    "        env.render()\n",
    "        \n",
    "        # Delay to slow down the movement for better observation\n",
    "        pygame.time.delay(350) \n",
    "    \n",
    "    # After the episode ends, print the score\n",
    "    print(f'Episode {episode} Score: {reward}')\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "**Comment:**\n",
    "\n",
    "After seeing the agent's actions via pygame, its clear the agent doesn't always take the most optimal path, to get the agent to do this I can add a penalty to the number of timesteps taken to reach the target. This is something I will implement by adding a penalty of 0.1 for every action which did not result in finding the target location.\n",
    "\n",
    "To do this I will add a else case into the if statement in the step function for reward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evalution\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much point in running the evaluate_policy since reward system is quite simple. Across all episodes the agent was able to get to the door and receive the reward of 1.\n",
    "\n",
    "Since there is no penalty there is no real incentive for the agent to learn the most optimal path to the exit door. To add penalty to the reward to force the agent to start finding the optimal path. I will be adding this in in the next iteration of my custom environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have successfully created a simple custom environment for Trick or ReTreat with rendering in pygame. \n",
    "\n",
    "After training the agent using PPO, it is clear the agent has learnt to escape the mansion but fails to find the most optimal path. In the next notebook I will be making the following changes to the environment:\n",
    "\n",
    "    - adding penalty for each timestep the agent does not reach its target(door).\n",
    "    - adding in ghosts and candies to see if the agent can find an optimal path while maximsing its rewards.\n",
    "    - implement Q-learning instead of PPO to deal with the added complexity.\n",
    "    - compare PPO model and Q-learning training to see which trains the agent better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trick_or_retreat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
